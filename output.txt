4.2.a
Value Function:
[[0.46089055 0.56655611 0.673289   0.56655611]
 [0.56655611 0.         0.7811     0.        ]
 [0.673289   0.7811     0.89       0.        ]
 [0.         0.89       1.         0.        ]]

Policy:
[[1 2 1 0]
 [1 0 1 0]
 [2 2 1 0]
 [0 2 2 0]]



4.2.b
Value Function:
[[-0.34726875 -0.24976641 -0.1512792  -0.05179717  0.04868972  0.15019164
   0.25271883  0.35628164]
 [-0.24976641 -0.1512792  -0.05179717  0.04868972  0.15019164  0.25271883
   0.35628164  0.46089055]
 [-0.1512792  -0.05179717  0.04868972  0.          0.25271883  0.35628164
   0.46089055  0.56655611]
 [-0.05179717  0.04868972  0.15019164  0.25271883  0.35628164  0.
   0.56655611  0.673289  ]
 [-0.1512792  -0.05179717  0.04868972  0.          0.46089055  0.56655611
   0.673289    0.7811    ]
 [-0.24976641  0.          0.          0.46089055  0.56655611  0.673289
   0.          0.89      ]
 [-0.1512792   0.          0.25271883  0.35628164  0.          0.7811
   0.          1.        ]
 [-0.05179717  0.04868972  0.15019164  0.          0.7811      0.89
   1.          0.        ]]

Policy:
[[2 2 2 2 2 1 1 1]
 [2 2 2 2 2 2 2 1]
 [1 2 1 0 1 2 2 1]
 [1 2 2 2 1 0 2 1]
 [2 2 3 0 2 2 2 1]
 [1 0 0 2 3 3 0 1]
 [0 0 2 3 0 1 0 1]
 [0 2 1 0 2 2 2 0]]


4.3.a
Yes, the agent can reach the goal. It is showing "Episode finished after 15 timesteps."


4.3.b
Policy:
[[2 2 2 2 2 2 2 1]
 [2 2 2 2 2 2 1 1]
 [1 1 1 0 2 2 2 1]
 [2 2 2 2 1 0 2 1]
 [2 2 3 0 2 2 2 1]
 [3 0 0 2 2 3 0 1]
 [1 0 2 0 0 1 0 1]
 [2 1 1 0 2 2 2 0]]

4.3.c
No, the SARSA policy is non-deterministic because we added the epsilon=0.2 and it is stochastic so the agent doesn't always take the same action.

4.3.d
No, the agent doesn't always succeed in reaching the goal. This happens because the epsilon greedy method makes the agent to take random actions and this could lead it to fall into the holes sometimes. Also the algorithm itself is not designed for always getting the optimal solution.